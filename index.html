<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  John  Thickstun


</title>
<meta name="description" content="John Thickstun's professional website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css">

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ¶</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://johnthickstun.com/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/thickstun_cv.pdf">
                CV
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/docs/">
                notes
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/service/">
                service
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">John</span>  Thickstun
    </h1>
     <p class="desc">Assistant Professor - <a href="https://www.cornell.edu/" target="_blank" rel="noopener noreferrer">Cornell University</a> - <a href="https://cs.cornell.edu/" target="_blank" rel="noopener noreferrer">Computer Science</a>.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <picture>
    
    <source media="(max-width: 480px)" srcset="/assets/resized/prof_pic-480x720.jpg">
    
    <source media="(max-width: 800px)" srcset="/assets/resized/prof_pic-800x1200.jpg">
    
    <source media="(max-width: 1400px)" srcset="/assets/resized/prof_pic-1400x2100.jpg">
    
    <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg" alt="prof_pic.jpg">
</source></source></source></picture>

      
      
    </div>
    

    <div class="clearfix">
      <p>I work on machine learning and generative models. Iâ€™m interested in methods that control the behavior of models, both from the perspective of a user who hopes to use a model to accomplish concrete tasks, and from the perspective of a model provider or policymaker who hopes to broadly regulate the outputs of a model. I am also interested in applications of generative models that push beyond the standard text and image modalities, including music technologies.</p>

<p>Previously I was a Postdoctoral Scholar at Stanford University, advised by <a href="https://cs.stanford.edu/~pliang/" target="_blank" rel="noopener noreferrer">Percy Liang</a>. I completed my PhD in the <a href="http://www.cs.washington.edu" target="_blank" rel="noopener noreferrer">Allen School of Computer Science &amp; Engineering</a> at the <a href="http://www.washington.edu/" target="_blank" rel="noopener noreferrer">University of Washington</a>, where I was co-advised by <a href="https://sham.seas.harvard.edu/" target="_blank" rel="noopener noreferrer">Sham Kakade</a> and <a href="https://faculty.washington.edu/zaid/" target="_blank" rel="noopener noreferrer">Zaid Harchaoui</a>. I studied Applied Mathematics as an undergraduate at <a href="https://www.brown.edu/" target="_blank" rel="noopener noreferrer">Brown University</a>, advised by <a href="http://cs.brown.edu/people/echarnia/" target="_blank" rel="noopener noreferrer">Eugene Charniak</a> and <a href="http://bjornsandstede.com/" target="_blank" rel="noopener noreferrer">BjÃ¶rn Sandstede</a>.</p>

<!--<b>Prospective students:</b> I am recruiting students to join my group starting Fall 2025! Please <a href="https://www.cs.cornell.edu/phd/admissions">apply</a> to the Cornell CS PhD program. I plan to admit multiple PhD students this year interested in generative models, music technologies, or both.-->

<p>The <a href="https://zenodo.org/record/5120004#.YXDPwKBlBpQ" target="_blank" rel="noopener noreferrer">MusicNet dataset</a> has moved to permanent hosting at Zenodo.</p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Jul 1, 2024</th>
          <td>
            
              I am joining <a href="https://www.cs.cornell.edu/" target="_blank" rel="noopener noreferrer">Cornell University</a> as an Assistant Professor of Computer Science, starting Fall 2024!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 17, 2024</th>
          <td>
            
              Hooktheory released <a href="https://www.hooktheory.com/hookpad/aria" target="_blank" rel="noopener noreferrer">Aria</a>: an AI co-creator for chords and melodies powered by the Anticipatory Music Transformer. Read about it on the <a href="https://www.audiocipher.com/post/hooktheory" target="_blank" rel="noopener noreferrer">AudioCipher Blog</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 18, 2024</th>
          <td>
            
              We <a href="https://huggingface.co/stanford-crfm/music-large-800k" target="_blank" rel="noopener noreferrer">released</a> a new, 780M parameter Anticipatory Music Transformer. Additional discussion <a href="https://twitter.com/jwthickstun/status/1769741258891661773" target="_blank" rel="noopener noreferrer">here</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 7, 2023</th>
          <td>
            
              <style>
.yt {
  position: relative;
  display: block;
  width: 90%; /* width of iframe wrapper */
  height: 0;
  margin: auto;
  padding: 0% 0% 56.25%; /* 16:9 ratio */
  overflow: hidden;
}
.yt iframe {
  position: absolute;
  top: 0; bottom: 0; left: 0;
  width: 100%;
  height: 100%;
  border: 0;
}
</style>

<a href="https://hai.stanford.edu/" target="_blank" rel="noopener noreferrer">Stanford HAI</a> featured my recent work on the Anticipatory Music Transformer!

<br><br>

<div class="yt">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/zDeKNUrbnNk" allowfullscreen=""></iframe>
</div>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 11, 2023</th>
          <td>
            
              Megha and I released human-LM interaction data that we collected last year for <a href="https://arxiv.org/abs/2212.09746" target="_blank" rel="noopener noreferrer">HALIE</a>. We wrote a <a href="https://crfm.stanford.edu/2023/10/11/halie.html" target="_blank" rel="noopener noreferrer">blog post</a> that documents the data release, and highlights some qualitative trends in the data that we found interesting

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TMLR</abbr>
    
  
  
  </div>

  <div id="kuditipudi2023robust" class="col-sm-8">
    
      <div class="title">Robust distortion-free watermarks for language models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Kuditipudi, Rohith,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hashimoto, Tatsunori,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Transactions on Machine Learning Research</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2307.15593" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/watermark" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://crfm.stanford.edu/2023/07/30/watermarking.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
      <a href="https://www.youtube.com/watch?v=jkwUUAnCI_A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers â€“ which we compute using a randomized watermark key â€“ to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models â€“ OPT-1.3B, LLaMA-7B and Alpaca-7B â€“ to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text (pâ‰¤0.01) from 35 tokens even after corrupting between 40-50% of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around 25% of the responses â€“ whose median length is around 100 tokens â€“ are detectable with pâ‰¤0.01, and the watermark is also less robust to certain automated paraphrasing attacks we implement.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TMLR</abbr>
    
  
  
  </div>

  <div id="thickstun2023anticipatory" class="col-sm-8">
    
      <div class="title">Anticipatory music transformer</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hall, David,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Donahue, Chris,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Transactions on Machine Learning Research</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2306.08620" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/anticipation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://crfm.stanford.edu/2023/06/16/anticipatory-music-transformer.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
      <a href="https://www.youtube.com/watch?v=zDeKNUrbnNk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Media</a>
    
    
      <a href="https://www.youtube.com/watch?v=b_vFfssGJNI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with similar musicality to even music composed by humans over a 20-second clip.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JMLR</abbr>
    
  
  
  </div>

  <div id="pillutla2023mauve" class="col-sm-8">
    
      <div class="title">MAUVE Scores for Generative Models: Theory and Practice</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pillutla, Krishna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Lang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Welleck, Sean,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Swayamdipta, Swabha,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zellers, Rowan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Oh, Sewoong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Choi, Yejin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Harchaoui, Zaid
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Machine Learning Research</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2212.14578" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/krishnap25/mauve" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generative AI has matured to a point where large-scale models can generate text that seems indistinguishable from human-written text and remarkably photorealistic images. Automatically measuring how close the distribution of generated data is to the target real data distribution is a key step in diagnosing existing models and developing better models. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore four approaches to statistically estimate these scores: vector quantization, non-parametric estimation, classifier-based estimation, and parametric Gaussian approximations. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of f-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We conclude the paper by demonstrating its applications to other AI domains and discussing practical recommendations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  
    <span class="award badge">Outstanding Paper</span>
  
  </div>

  <div id="hewitt2023backpack" class="col-sm-8">
    
      <div class="title">Backpack language models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Hewitt, John,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manning, Christopher D.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Association for Computational Linguistics</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2305.16765" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/john-hewitt/backpacks-flash-attn" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="http://backpackmodels.science" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the modelâ€™s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124M parameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LMâ€™s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Neurips</abbr>
    
  
  
    <span class="award badge">Oral Presentation</span>
  
  </div>

  <div id="li2022diffusion" class="col-sm-8">
    
      <div class="title">Diffusion-LM improves controllable text generation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Li, Xiang Lisa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gulrajani, Ishaan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liang, Percy,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hashimoto, Tatsunori B.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2205.14217" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/XiangLi1999/Diffusion-LM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      
      <a href="/assets/pdf/diffusion-lm.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Neurips</abbr>
    
  
  
    <span class="award badge">Outstanding Paper</span>
  
  </div>

  <div id="pillutla2021mauve" class="col-sm-8">
    
      <div class="title">MAUVE: measuring the gap between neural text and human text using divergence frontiers</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pillutla, Krishna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Swayamdipta, Swabha,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zellers, Rowan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Welleck, Sean,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Choi, Yejin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Harchaoui, Zaid
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2102.01454" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/krishnap25/mauve-experiments" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      
      <a href="https://krishnap25.github.io/slides/mauve.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  </div>

  <div id="jayaram2021parallel" class="col-sm-8">
    
      <div class="title">Parallel and flexible sampling from autoregressive models via Langevin dynamics</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jayaram, Vivek,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Thickstun, John</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2105.08164" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/vivjay30/pnf-sampling/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/thickstun_langevin_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
      <a href="https://grail.cs.washington.edu/projects/pnf-sampling/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  </div>

  <div id="jayaram2020source" class="col-sm-8">
    
      <div class="title">Source separation with deep generative priors</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jayaram, Vivek,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Thickstun, John</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2002.07942" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/basis-separation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/thickstun_jayaram_sourcesep_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  
    <span class="award badge">Oral Presentation</span>
  
  </div>

  <div id="thickstun2018invariances" class="col-sm-8">
    
      <div class="title">Invariances and data augmentation for supervised music transcription</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Foster, Dean P,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kakade, Sham M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Acoustics, Speech and Signal Processing</em>
      
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1711.04845" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/thickstun2018invariances" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  </div>

  <div id="thickstun2017learning" class="col-sm-8">
    
      <div class="title">Learning features of music from scratch</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kakade, Sham M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1611.09827" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/thickstun2017learning" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/thickstun2017learning_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
      <a href="https://zenodo.org/record/5120004#.YaaHUlOIZpR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6A%74%68%69%63%6B%73%74%75%6E@%63%6F%72%6E%65%6C%6C.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=RkuzIZMAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/jthickstun" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/john-thickstun-87779865" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/jwthickstun" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>













      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2025 John  Thickstun.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
