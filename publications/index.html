<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  John  Thickstun


  | publications

</title>
<meta name="description" content="John Thickstun's professional website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css">

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ¶</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://johnthickstun.com/publications/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://johnthickstun.com/">
       <span class="font-weight-bold">John</span>   Thickstun
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/thickstun_cv.pdf">
                CV
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/docs/">
                notes
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/service/">
                service
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">Publications in reversed chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2024</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TMLR</abbr>
    
  
  
  </div>

  <div id="kuditipudi2023robust" class="col-sm-8">
    
      <div class="title">Robust distortion-free watermarks for language models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Kuditipudi, Rohith,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hashimoto, Tatsunori,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Transactions on Machine Learning Research</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2307.15593" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/watermark" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://crfm.stanford.edu/2023/07/30/watermarking.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
      <a href="https://www.youtube.com/watch?v=jkwUUAnCI_A" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers â€“ which we compute using a randomized watermark key â€“ to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models â€“ OPT-1.3B, LLaMA-7B and Alpaca-7B â€“ to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text (pâ‰¤0.01) from 35 tokens even after corrupting between 40-50% of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around 25% of the responses â€“ whose median length is around 100 tokens â€“ are detectable with pâ‰¤0.01, and the watermark is also less robust to certain automated paraphrasing attacks we implement.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TMLR</abbr>
    
  
  
  </div>

  <div id="thickstun2023anticipatory" class="col-sm-8">
    
      <div class="title">Anticipatory music transformer</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hall, David,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Donahue, Chris,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Transactions on Machine Learning Research</em>
      
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2306.08620" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/anticipation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://crfm.stanford.edu/2023/06/16/anticipatory-music-transformer.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
      <a href="https://www.youtube.com/watch?v=zDeKNUrbnNk" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Media</a>
    
    
      <a href="https://www.youtube.com/watch?v=b_vFfssGJNI" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Talk</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with similar musicality to even music composed by humans over a 20-second clip.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JMLR</abbr>
    
  
  
  </div>

  <div id="pillutla2023mauve" class="col-sm-8">
    
      <div class="title">MAUVE Scores for Generative Models: Theory and Practice</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pillutla, Krishna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Lang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Welleck, Sean,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Swayamdipta, Swabha,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zellers, Rowan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Oh, Sewoong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Choi, Yejin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Harchaoui, Zaid
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Machine Learning Research</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2212.14578" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/krishnap25/mauve" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generative AI has matured to a point where large-scale models can generate text that seems indistinguishable from human-written text and remarkably photorealistic images. Automatically measuring how close the distribution of generated data is to the target real data distribution is a key step in diagnosing existing models and developing better models. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore four approaches to statistically estimate these scores: vector quantization, non-parametric estimation, classifier-based estimation, and parametric Gaussian approximations. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of f-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We conclude the paper by demonstrating its applications to other AI domains and discussing practical recommendations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  
    <span class="award badge">Outstanding Paper</span>
  
  </div>

  <div id="hewitt2023backpack" class="col-sm-8">
    
      <div class="title">Backpack language models</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Hewitt, John,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manning, Christopher D.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Association for Computational Linguistics</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2305.16765" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/john-hewitt/backpacks-flash-attn" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="http://backpackmodels.science" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present Backpacks: a new neural architecture that marries strong modeling performance with an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination of sense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the modelâ€™s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124M parameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LMâ€™s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TMLR</abbr>
    
  
  
  </div>

  <div id="lee2022evaluating" class="col-sm-8">
    
      <div class="title">Evaluating Human-Language Model Interaction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Lee, Mina,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Srivastava, Megha,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hardy, Amelia,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Durmus, Esin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Paranjape, Ashwin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gerard-Ursin, Ines,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Xiang Lisa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ladhak, Faisal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Rong, Frieda,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Rose E.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kwon, Minae,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Park, Joon Sung,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Cao, Hancheng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lee, Tony,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bommasani, Rishi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bernstein, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Transactions on Machine Learning Research</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2212.09746" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/minggg/human-lm-interaction" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Many real-world applications of language models (LMs), such as code autocomplete and writing assistance, involve human-LM interaction, but the main LM benchmarks are non-interactive, where a system produces output without human intervention. To evaluate human-LM interaction, we develop a framework, Human-AI Language-based Interaction Evaluation (H-LINE), that expands non-interactive evaluation along three dimensions, capturing (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality. We then design five tasks ranging from goal-oriented to open-ended to capture different forms of interaction. On four state-of-the-art LMs (three variants of OpenAIâ€™s GPT-3 and AI21â€™s J1-Jumbo), we find that non-interactive performance does not always result in better human-LM interaction and that first-person and third-party metrics can diverge, suggesting the importance of examining the nuances of human-LM interaction.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ISMIR</abbr>
    
  
  
  </div>

  <div id="donahue2022melody" class="col-sm-8">
    
      <div class="title">Melody transcription via generative pre-training</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Donahue, Chris,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liang, Percy
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Society for Music Information Retrieval</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2212.01884" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/chrisdonahue/sheetsage" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://chrisdonahue.com/sheetsage" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite the central role that melody plays in music perception, it remains an open challenge in music information retrieval to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in melody transcription is building methods which can handle broad audio containing any number of instrument ensembles and musical styles - existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data - we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build Sheet Sage, a system capable of transcribing human-readable lead sheets directly from music audio.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">bioRxiv</abbr>
    
  
  
  </div>

  <div id="benster2022reconstruction" class="col-sm-8">
    
      <div class="title">Reconstruction of visual images from mouse retinal ganglion cell spiking activity using convolutional neural networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Benster, Tyler,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Babino, Darwin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hunt, Matthew,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Xiyang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Oh, Sewoong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gelder, Russell N Van
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://www.biorxiv.org/content/10.1101/2022.06.10.482188v1.abstract" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/tbenst/2021-retina-reconstructions" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>All visual information in mammals is encoded in the aggregate pattern of retinal ganglion cell (RGC) firing. How this information is decoded to yield percepts remains incompletely understood. We have trained convolutional neural networks with multielectrode array-recorded murine RGC responses to projected images. The trained model accurately reconstructed novel facial images solely from RGC firing data. In this model, subpopulations of cells with faster firing rates are largely sufficient for accurate reconstruction, and ON- and OFF-cells contribute complementary and overlapping information to image reconstruction. Information content for reconstruction correlates with overall firing rate, and locality of information contributing to reconstruction varies substantially across the image and retina. This model demonstrates that artificial neural networks are capable of learning multicellular sensory neural encoding, and provides a viable model for understanding visual information encoding.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Neurips</abbr>
    
  
  
    <span class="award badge">Oral Presentation</span>
  
  </div>

  <div id="li2022diffusion" class="col-sm-8">
    
      <div class="title">Diffusion-LM improves controllable text generation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Li, Xiang Lisa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gulrajani, Ishaan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liang, Percy,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hashimoto, Tatsunori B.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2205.14217" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/XiangLi1999/Diffusion-LM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      
      <a href="/assets/pdf/diffusion-lm.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Dissertation</abbr>
    
  
  
  </div>

  <div id="thickstun2021leveraging" class="col-sm-8">
    
      <div class="title">Leveraging generative models for music and signal processing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Thickstun, John</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>University of Washington</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="/assets/pdf/thickstun_thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Neurips</abbr>
    
  
  
    <span class="award badge">Outstanding Paper</span>
  
  </div>

  <div id="pillutla2021mauve" class="col-sm-8">
    
      <div class="title">MAUVE: measuring the gap between neural text and human text using divergence frontiers</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Pillutla, Krishna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Swayamdipta, Swabha,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zellers, Rowan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Welleck, Sean,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Choi, Yejin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Harchaoui, Zaid
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2102.01454" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/krishnap25/mauve-experiments" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      
      <a href="https://krishnap25.github.io/slides/mauve.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  </div>

  <div id="jayaram2021parallel" class="col-sm-8">
    
      <div class="title">Parallel and flexible sampling from autoregressive models via Langevin dynamics</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jayaram, Vivek,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Thickstun, John</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2105.08164" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/vivjay30/pnf-sampling/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/thickstun_langevin_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
      <a href="https://grail.cs.washington.edu/projects/pnf-sampling/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">L4DC</abbr>
    
  
  
  </div>

  <div id="ainsworth2021faster" class="col-sm-8">
    
      <div class="title">Faster policy learning with continuous-time gradients</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Ainsworth, Samuel K.,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lowrey, Kendall,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Srinivasa, Siddhartha S.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Learning for Dynamics and Control</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2012.06684" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/samuela/ctpg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the estimation of policy gradients for continuous-time systems with known dynamics. By reframing policy learning in continuous-time, we show that it is possible construct a more efficient and accurate gradient estimator. The standard back-propagation through time estimator (BPTT) computes exact gradients for a crude discretization of the continuous-time system. In contrast, we approximate continuous-time gradients in the original system. With the explicit goal of estimating continuous-time gradients, we are able to discretize adaptively and construct a more efficient policy gradient estimator which we call the Continuous-Time Policy Gradient (CTPG). We show that replacing BPTT policy gradients with more efficient CTPG estimates results in faster and more robust learning in a variety of control tasks and simulators.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  
  </div>

  <div id="thickstun2020rethinking" class="col-sm-8">
    
      <div class="title">Rethinking evaluation methodology for audio-to-score alignment</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Brennan, Jennifer,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Verma, Harsh
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>arXiv preprint arXiv:2009.14374</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2009.14374" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/alignment-eval" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper offers a precise, formal definition of an audio-to-score alignment. While the concept of an alignment is intuitively grasped, this precision affords us new insight into the evaluation of audio-to-score alignment algorithms. Motivated by these insights, we introduce new evaluation metrics for audio-to-score alignment. Using an alignment evaluation dataset derived from pairs of KernScores and MAESTRO performances, we study the behavior of our new metrics and the standard metrics on several classical alignment algorithms.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  
  </div>

  <div id="paranjape2020information" class="col-sm-8">
    
      <div class="title">An information bottleneck approach for controlling conciseness in rationale extraction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Paranjape, Bhargavi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Joshi, Mandar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hajishirzi, Hannaneh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zettlemoyer, Luke
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Conference on Empirical Methods in Natural Language Processing</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2005.00652" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/bhargaviparanjape/explainable_qa" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      
      <a href="https://bhargaviparanjape.github.io/documents/slides/EMNLP2020_SPARSE_PRIORS.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Decisions of complex language understanding models can be rationalized by limiting their inputs to a relevant subsequence of the original text. A rationale should be as concise as possible without significantly degrading task performance, but this balance can be difficult to achieve in practice. In this paper, we show that it is possible to better manage this trade-off by optimizing a bound on the Information Bottleneck (IB) objective. Our fully unsupervised approach jointly learns an explainer that predicts sparse binary masks over sentences, and an end-task predictor that considers only the extracted rationale. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on ERASER benchmark tasks demonstrate significant gains over norm-minimization techniques for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples) closes the gap with a model that uses the full input.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  
  </div>

  <div id="jayaram2020source" class="col-sm-8">
    
      <div class="title">Source separation with deep generative priors</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jayaram, Vivek,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Thickstun, John</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Machine Learning</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2002.07942" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/basis-separation" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/thickstun_jayaram_sourcesep_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ISMIR</abbr>
    
  
  
  </div>

  <div id="verma2019convolutional" class="col-sm-8">
    
      <div class="title">Convolutional composer classification</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Verma, Harsh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Thickstun, John</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Society for Music Information Retrieval</em>
      
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1911.11737" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/harshshredding/Convolutional-Composer-Classification" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/verma_composers_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper investigates end-to-end learnable models for attributing composers to musical scores. We introduce several pooled, convolutional architectures for this task and draw connections between our approach and classical learning approaches based on global and n-gram features. We evaluate models on a corpus of 2,500 scores from the KernScores collection, authored by a variety of composers spanning the Renaissance era to the early 20th century. This corpus has substantial overlap with the corpora used in several previous, smaller studies; we compare our results on subsets of the corpus to these previous works.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ISMIR</abbr>
    
  
  
  </div>

  <div id="thickstun2018coupled" class="col-sm-8">
    
      <div class="title">Coupled recurrent models for polyphonic music composition</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Foster, Dean P,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kakade, Sham M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Society for Music Information Retrieval</em>
      
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1811.08045" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/ismir2019coupled" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/thickstun_coupled_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces a novel recurrent model for music composition that is tailored to the structure of polyphonic music. We propose an efficient new conditional probabilistic factorization of musical scores, viewing a score as a collection of concurrent, coupled sequences: i.e. voices. To model the conditional distributions, we borrow ideas from both convolutional and recurrent neural models; we argue that these ideas are natural for capturing musicâ€™s pitch invariances, temporal structure, and polyphony. We train models for single-voice and multi-voice composition on 2,300 scores from the KernScores dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICASSP</abbr>
    
  
  
    <span class="award badge">Oral Presentation</span>
  
  </div>

  <div id="thickstun2018invariances" class="col-sm-8">
    
      <div class="title">Invariances and data augmentation for supervised music transcription</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Foster, Dean P,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kakade, Sham M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Acoustics, Speech and Signal Processing</em>
      
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1711.04845" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/thickstun2018invariances" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper explores a variety of models for frame-based music transcription, with an emphasis on the methods needed to reach state-of-the-art on human recordings. The translation-invariant network discussed in this paper, which combines a traditional filterbank with a convolutional neural network, was the top-performing model in the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. This class of models shares parameters in the log-frequency domain, which exploits the frequency invariance of music to reduce the number of model parameters and avoid overfitting to the training data. All models in this paper were trained with supervision by labeled data from the MusicNet dataset, augmented by random label-preserving pitch-shift transformations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MIREX</abbr>
    
  
  
  </div>

  <div id="thickstun2017mirex" class="col-sm-8">
    
      <div class="title">Frequency domain convolutions for multiple F0 estimation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Foster, Dean P,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kakade, Sham M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://www.music-ir.org/mirex/abstracts/2017/THK1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This document describes the THK1 submission in the 2017 MIREX Multi-F0 competition. The model is a convolutional neural network, trained using the MusicNet labels. Its input is a bank of logarithmically-spaced frequency filters. These filters exhibit translation invariance along the log-frequency axis, which is captured in this model by one-dimensional convolutions along the frequency axis. The model fully connects across the time axis to capture temporal dependencies. The training data is augmented by pitch-shifting the original data by up to 5 semitones and applying small (up to 1/10 semitone) continuous pitch jitter to the input.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  
  </div>

  <div id="thickstun2017learning" class="col-sm-8">
    
      <div class="title">Learning features of music from scratch</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Thickstun, John</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Harchaoui, Zaid,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Kakade, Sham M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations</em>
      
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/1611.09827" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/jthickstun/thickstun2017learning" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/pdf/thickstun2017learning_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
      <a href="https://zenodo.org/record/5120004#.YaaHUlOIZpR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2025 John  Thickstun.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
